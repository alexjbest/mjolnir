\author{Based on lectures by Dr. Matthias Englert\\
\small{Notes by Alex J. Best}}
\date{\today}
\title{CS301 Complexity of Algorithms Notes}
\documentclass[11pt,a4paper]{article}
\usepackage{amsmath, amssymb, amsfonts, fullpage, amsthm, enumerate}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{claim}{Claim}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{defns}{Definitions}
\newtheorem{prob}{Problem}
\newtheorem{ex}{Example}
\newtheorem{rem}{Remark}
\newtheorem{nota}{Notation}
\newtheorem{alg}{Algorithm}

\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\U}{\mathcal{U}}

\begin{document}
\maketitle

\paragraph{Introduction}
These are some rough notes put together for CS301 in 2014 to make it a little easier to revise.
The headings correspond roughly to the contents of the module that is on the module webpage so hopefully these are fairly complete, however they are not guaranteed to be.

\paragraph{What is a problem?}
\begin{defn}
A \emph{problem} is a function
\[
f\colon\{0,1\}^*\to\{0,1\}^*.
\]
A \emph{decision problem} is a function
\[
f\colon\{0,1\}^*\to\{0,1\}.
\]
We identify a decision problem $f$ with the language
\[
L_f = \{x : f(x) = 1\}.
\]
and call the problem of computing $f$ the problem of deciding the language $L_f$.
\end{defn}

\paragraph{What is a computation?}
A set of fixed mechanical rules for computing a function for any input.

\paragraph{Definition of a Turing machine.}
\begin{defn}
A \emph{Turing machine} consists of an infinite tape with letters of the alphabet $\Gamma$ (usually $=\{0,1,\square\}$) written on it.
At the start the input is written on the tape beginning at the head and the state is $q_\text{start}$.
The transition function
\[
\delta\colon Q\times \Gamma \to Q\times \Gamma \times \{L,S,R\}
\]
dictates what to read, which state to switch to and which direction to move to after reading a symbol while in a given state.
So the machine is given by $(\Gamma,Q,\delta)$.
When the machine reaches the state $q_\text{end}$ the computation halts and the output is the section of tape starting at the head until the first blank $\square$.
\end{defn}

\begin{defn}
A Turing machine $M$ computes a function $f\colon \{0,1\}^* \to \{0,1\}^*$ in time $T\colon \NN \to \NN$ if for every $x\in \{0,1\}^*$ the output of $M$ when given $x$ initially is $f(x)$, this computation must finish after at most $T(|x|)$ steps (this implies $M$ halts on every input).

We say $M$ computes $f$ if it computes $f$ in $T(n)$ time for any function $T$.
\end{defn}
%TODO maybe examples?

\paragraph{What happens if we increase the alphabet?}
\begin{claim}
Let $f\colon\{0,1\}^*\to\{0,1\}^*$ and $T\colon \NN \to \NN$ be some functions.
If $f$ is computable in time $T(n)$ by a Turing machine $M$ using alphabet $\Gamma$ then $f$ is computable in time
\[
3\lceil\log|\Gamma|\rceil T(n)
\]
by a Turing machine $\tilde{M}$ that uses only the alphabet $\{0,1,\square\}$.
\end{claim}
To see this we can encode all the old symbols in terms of only $\{0,1,\square\}$ and create a new Turing machine that does what the old one would do, we have to move back an fourth along a strip of size $\log|\Gamma|$ at most 3 times to do this however.

\paragraph{$k$-tape Turing machines.}
We can also using Turing machines that have $k$ tapes and $k$ heads rather than simply one however this doesn't make much difference either.
\begin{claim}
Let $f\colon\{0,1\}^*\to\{0,1\}^*$ and $T\colon \NN \to \NN$ be some functions.
If $f$ is computable in time $T(n)$ by a $k$-tape Turing machine $M$ using then $f$ is computable in time
\[
7kT(n)^2
\]
by a single tape Turing machine $\tilde{M}$.
\end{claim}

\paragraph{Church-Turing thesis.}
The Church-Turing thesis is the statement that
\begin{quote}
Every physically realisable computation device (be it silicon-based, DNA-based, neuron based, ...) can be simulated by a Turing machine.
\end{quote}
This is generally believed to be true.

\paragraph{Universal Turing machines.}
As a Turing machine is given by some finite amount of data we can represent it by some string encoding.
We assume that we have picked an encoding so that every string represents a Turing machine and every Turing machine can be encoded by infinitely many strings.
The Turing machine encoded by a string $\alpha$ is denoted $M_\alpha$.
\begin{thm}
There exists a Turing machine (a universal Turing machine) that when given a pair $\langle \alpha,x\rangle$ as input outputs the result of running the Turing machine encoded by $\alpha$ with input $x$.
Moreover if the machine encoded by $\alpha$ halts within $T(|x|)$ steps on input $x$ then the universal Turing machine halts within $C\operatorname{poly}(T(|x|))$ steps, where $C$ is independent of $|x|$.
\end{thm}

We assume we have fixed some encoding for Turing machines and a corresponding universal Turing machine, denoted $\U$.

\paragraph{Uncomputable functions.}
The set of all functions
\[
f\colon\{0,1\}^*\to\{0,1\}
\]
is uncountable.
So as any Turing machine can be encoded as a finite string of bits under some fixed encoding, there are countably many Turing machines.
\begin{cor}
So there are functions
\[
f\colon\{0,1\}^*\to\{0,1\}
\]
that are not computable.
\end{cor}

\begin{ex}\label{firstuncomp}
The function
\[
f(\alpha) =\begin{cases} 0 & M_\alpha (\alpha) = 1,\\
1 &\text{otherwise}
\end{cases}
\]
is not computable.

This is as if we had a Turing machine $M$ that computed $f$ then $M$ halts for all $x$, with $M(x) = f(x)$.
So if we let $x$ be a string representing the Turing machine $M$ then $M(x) = f(x)$, but this is a contradiction by the definition of $f$.
\end{ex}

\paragraph{HALT is not computable.}
Another function that is not computable is
\[
\text{HALT}(\langle \alpha,x\rangle) =\begin{cases}
1 & M_\alpha \text{ halts on input }x,\\
0 &\text{otherwise}.
\end{cases}
\]
\begin{proof}
Assume that $M_{\text{HALT}}$ is a Turing machine that computes HALT, then we can design a Turing machine to compute the function $f$ in example~\ref{firstuncomp}, thus deriving a contradiction.

The Turing machine for $f$ would first run $M_{\text{HALT}}(\langle \alpha,\alpha\rangle)$ outputting 0 if this returned 1.
Otherwise it would then use the universal Turing machine $\U$ to compute $M_\alpha (\alpha)$, outputting 1 if this returned 0 and 0 otherwise.
\end{proof}

There are more functions that are practically useful that are not computable.
For example deciding if a Diophantine equation (a possibly multivariate polynomial with integer coefficients) has a solution in the integers is impossible in general.

The language
\[
\{\alpha : M_{\alpha} \text{ halts on all inputs}\}
\]
is also undecidable.
This is as if we could compute this language we could compute HALT by taking a pair $\langle \alpha,x\rangle$ and forming a Turing machine the always computes $M_\alpha(x)$ no matter what input it is given.
If we could decide if the new Turing machine halted on all inputs we could decide if $M_\alpha$ halts on input $x$.

\paragraph{Rice's Theorem.}
All Turing machines correspond to a function
\[
f\colon\{0,1\}^*\to\{0,1\}^*\cup\{\bot\}
\]
where $f(x) = \bot$ means that the Turing machine does not halt on input $x$.
Not all of these functions correspond to Turing machines, but we can take $\mathcal{R}$ to be the set of all such functions that do correspond to Turing machines.

\begin{thm}[Rice's Theorem]
Let $\mathcal{C}$ be a non-empty proper subset of $\mathcal{R}$, then the language
\[
\{\alpha : M_\alpha \text{ corresponds to a function } f\in \mathcal{C}\}
\]
is undecidable.
\end{thm}
\begin{proof}
%TODO
\end{proof}

\paragraph{The complexity class P.}
We now define some \emph{complexity classes}, sets of functions that can be computed with some given resources.
\begin{defn}[The class DTIME]
Let $T\colon \NN\to\NN$ be a function, then we let $\text{DTIME}(T(n))$ be the set of boolean functions computable in $O(T(n))$ time.
\end{defn}
\begin{defn}[The class P]
\[
\text{P} = \bigcup_{k\ge 1} \text{DTIME}(n^k).
\]
This class does not depend on the exact definition of Turing machine used.
Problems in P are thought of as efficiently solvable and are a very natural model for this concept.
\end{defn}

\paragraph{Strong Church-Turing thesis.}
The strong Church-Turing thesis is the statement that
\begin{quote}
Every physically realisable computation device (be it silicon-based, DNA-based, neuron based, ...) can be simulated by a Turing machine \emph{with only a polynomial overhead}.
\end{quote}
This is more controversial than the normal Church-Turing thesis as either
\begin{enumerate}[a)]
\item quantum mechanics does not behave as we currently understand it to,
\item a classical computer can factor integer's in polynomial time or
\item the strong Church-Turing thesis is wrong.
\end{enumerate}

\paragraph{Reductions.}
\begin{defn}[Karp reduction]
A language $L\subseteq\{0,1\}^*$ is \emph{Karp reducible} to a language $L'\subseteq \{0,1\}^*$ if there exists a computable function 
\[
f\colon \{0,1\}^* \to \{0,1\}^*,
\]
such that for all $x$ 
\[
x\in L \iff f(x) \in L'.
\]
\end{defn}

\begin{defn}[Polynomial time Karp reduction]
A language $L\subseteq\{0,1\}^*$ is \emph{polynomial time} Karp reducible to a language $L'\subseteq \{0,1\}^*$ if there exists a \emph{polynomial time} computable function 
\[
f\colon \{0,1\}^* \to \{0,1\}^*,
\]
such that for all $x$ 
\[
x\in L \iff f(x) \in L'.
\]
We denote this relationship by 
\[
L\le_p L'.
\]

If $L\le_p L'$ and $L'\le_p L$ then we write $L\equiv_p L'$.
\end{defn}

\paragraph{Vertex Cover and Independent Set are equivalent.}
INDEPENDENT SET: Given a graph $G = (V,E)$ and an integer $k$ is there set $S\subset V$ of size at least $k$ where each edge of $G$ has at most one endpoint in $S$.\\
VERTEX COVER: Given a graph $G = (V,E)$ and an integer $k$ is there set $S\subset V$ of size at most $k$ where each edge of $G$ has at least one endpoint in $S$.

\begin{claim}
VERTEX COVER $\equiv_p$ INDEPENDENT SET.
\end{claim}
\begin{proof}
$S$ is an independent set if and only if $V\setminus S$ is a vertex cover.
\end{proof}

\paragraph{Vertex Cover reduces to Set Cover.}
SET COVER: Given a set $U$, a collection $S_1,\ldots,S_m$ of subsets of $U$ and an integer $k$, does there exist a collection of $\le k$ of the subsets whose union is all of $U$.
\begin{claim}
VERTEX COVER $\le_p$ SET COVER.
\end{claim}
\begin{proof}
We create a set cover instance for a graph $G = (E,V)$ by letting $U = E$ and $S_v = \{e\in E : e \text{ is incident to } v\}$ be our collection of subsets.
Then there exists a set cover of size at most $k$ if and only if there exists a vertex cover of size at most $k$ in the graph $G$.
\end{proof}

\paragraph{3-SAT reduces to Independent Set.}
\begin{defn}
A \emph{literal} is a boolean variable or its negation.\\
A \emph{clause} is a disjunction (OR) of literals.\\
A propositional formula $\Phi$ is in \emph{conjunctive normal form} if it is a conjunction (AND) of clauses.\\
The problem SAT asks if a given propositional formula $\Phi$ that is in CNF has a satisfying assignment of variables.\\
A special case of this is 3-SAT where we require that each clause be a disjunction of exactly three literals.
\end{defn}
\begin{claim}
3-SAT $\le_p$ INDEPENDENT SET.
\end{claim}
\begin{proof}
We use the given proposition formula to construct an instance of INDEPENDENT SET as follows.
For each clause we add a triangle of vertices to a graph $G$, labelled by each literal.
We then connect all literals appearing to all of their negations.
Then $G$ contains an independent set of size $k$ if and only if $\Phi$ is satisfiable.
\end{proof}

\paragraph{Transitivity for the reducibility-relation.}
Reduction is transitive, i.e. if $X \le_p Y$ and $Y \le_p Z$ then $X\le_p Z$.
To see this we can think of composing the functions that provide the reductions from $X$ to $Y$ and $Y$ to $Z$.
\begin{ex}
\[
3\text{-SAT}\le_p \text{INDEPENDENT-SET} \le_p \text{VERTEX-COVER} \le_p \text{SET-COVER}
\]
\end{ex}

\paragraph{Definition of NP.}
For comparison we give an equivalent definition of the class P to the one given above.
\begin{defn}[Complexity class P]
A language $L$ is in the class P if there exists a Turing machine $M$ and polynomial $T$ so that $M$ terminates on input $x$ after at most $T(|x|)$ steps and $M$ accepts $x$ if and only if $x\in L$.
\end{defn}
\begin{defn}[Complexity class NP]
A language $L$ is in the class NP if there exists a Turing machine $M$ and polynomials $T$ and $p$ so that $M$ terminates on any input $x$ after at most $T(|x|)$ steps.
We also require that if $x\in L$ then there exists a certificate $t\in\{0,1\}^{p(|x|)}$ so that $M$ accepts $\langle x, t\rangle$, conversely if $x\not\in L$ we require that $M$ rejects any pair $\langle x,t\rangle$ where $t\in\{0,1\}^{p(|x|)}$.
\end{defn}

In the definition of NP above $M$ is called a certifier or verifier and $t$ a certificate or proof for $x$.
\begin{claim}
$\text{P}\subseteq \text{NP}$
\end{claim}
\begin{proof}
Take the certifier $M$ to be the decider for the problem from P, ignoring the second element of the input pair.
\end{proof}

\begin{ex}
The language
\[
\text{COMPOSITES} = \{s\in \NN : s \text{ is composite}\}
\]
is in NP.
This is as we can use a non-trivial proper factor as a certificate, and then the certifier just needs to check the factor is as claimed by checking bounds and dividing.
Here $|t| \le |s|$.
\end{ex}

\begin{ex}
The problem SAT is in NP as we can take a satisfying assignment of the $n$ boolean variables in the formula as the certificate.
The verifier then needs only run through the formula checking that each clause has at least one true literal.
\end{ex}

\begin{ex}
Another problem in NP is HAM-CYCLE which asks if a given graph $G = (V,E)$ has a simple cycle that visits every node.
The certificate for this problem is a permutation of the nodes of $G$ and so the certifier need only check that all nodes are in this permutation exactly once and that there are edges between consecutive nodes in the permutation.
\end{ex}

\paragraph{Definition of NP-completeness.}
\begin{defn}[NP-completeness]
A (decision) problem $Y$ is called \emph{NP-complete} if it is in NP and has the property that every other problem $X$ in NP reduces to it in polynomial time.
\end{defn}

\begin{thm}
Let $Y$ be an NP-complete problem, then $Y$ is solvable in polynomial time if and only if P $=$ NP.
\end{thm}
\begin{proof}
($\Leftarrow$) If P $=$ NP then $Y$ can be solved in polynomial time as $Y$ is in NP.\\
($\Rightarrow$) If $Y$ can be solved in polynomial time then as any problem $X$ in NP reduces to $Y$ in polynomial time we can solve $X$ is polynomial time.
Hence NP $\subseteq$ P and so P $=$ NP.
\end{proof}

\paragraph{An easy artificial NP-complete problem: TMSAT.}
%TODO

\paragraph{Alternative definition of NP using reductions and a representative (i.e. complete) problem of the class.}
Given a natural NP-complete problem we can equivalently define the complexity class NP to be all problems polynomial time reducible to that problem.

\paragraph{Cook-Levin theorem.}
\begin{thm}[Cook, Levin]
SAT is NP-complete.
\end{thm}
\begin{proof}
We know SAT is in NP, in order to show that all other problems in NP reduce to it we need some more machinery!
\end{proof}

\paragraph{Oblivious Turing machines.}
\begin{defn}
A Turing machine is called \emph{oblivious} is its head movements do not depend on the input $x$ but only on the length $|x|$ of the input.
\end{defn}

\begin{thm}
Given any Turing machine $M$ that decides a language in time $T_M(n)$ there exists an oblivious Turing machine that decides the same language in time $O(T_m(n)^2)$.
\end{thm}
\begin{proof}
Exercise!
\end{proof}

\paragraph{Proof of the Cook-Levin theorem.}
We will solve the problem of certificate ex %TODO

Now we have established a natural NP-complete problem others are easier to do.
To establish the NP-completeness of a given problem $Y$ we can perform the following
\begin{itemize}
\item[Step 1] Show $Y$ is in NP.
\item[Step 2] Choose an appropriate NP-complete problem $X$.
\item[Step 3] Prove $X\le_p Y$.
\end{itemize}

A major problem of complexity theory is if P $=$ NP, if it does then there are efficient algorithms for all NP-complete problems.
If not no such algorithms are possible, and most computer scientists believe this to be the case.
Most NP problems are known to be in P or NP-complete, factoring and graph isomorphism are exceptions.

\paragraph{3-SAT is NP-complete.}
\begin{thm}
3-SAT is NP-complete.
\end{thm}
\begin{proof}
It suffices to show that SAT $\le_p$ 3-SAT since we know that 3-SAT is in NP.%TODO
\end{proof}

\paragraph{Subset-Sum is NP-complete.}

\paragraph{Scheduling With Release Times is NP-complete.}

\paragraph{Hamiltonian cycle is NP-complete.}

\paragraph{TSP is NP-complete.}

\paragraph{3-Colouring is NP-complete.}

\paragraph{Planar 3-Coloring is NP-complete.}

\paragraph{Planar $k$-Coloring.}

\paragraph{Oracle Turing Machines.}

\paragraph{Cook-reductions.}

\paragraph{Oracle Turing Machines.}

\paragraph{Cook-reductions.}

\paragraph{Self-Reducibility.}

\paragraph{Gödel's first incompleteness Theorem.}

\paragraph{Test your intuition - complexity of (Longest Path, Shortest Path, Perfect Matching, MaxCut, Halt within 4000 steps, Min spanning tree, Degree bounded min spanning tree).}

\paragraph{Where does TAUTOLOGY fit?}

\paragraph{Asymmetry of NP.}

\paragraph{NP versus coNP.}

\paragraph{Some properties of NP and coNP.}

\paragraph{Well characterized problems.}

\paragraph{PRIMES is in NP.}

\paragraph{FACTOR is well characterized.}

\paragraph{PSPACE.}

\paragraph{QSAT is in PSPACE.}

\paragraph{NP is a subset of PSPACE.}

\paragraph{PSPACE is a subset of EXPTIME.}

\paragraph{Competitive Facility Location is PSPACE-complete.}

\paragraph{Sliding Blocks.}

\paragraph{Randomization.}

\paragraph{RP.}

\paragraph{Probability Amplification.}

\paragraph{coRP.}

\paragraph{Polynomial equality.}
\paragraph{Polynomial identity testing.}

\paragraph{BPP.}

\paragraph{Probability Amplification for BPP.}

\paragraph{ZPP.}

\paragraph{ZPP$=$RP $\cap$ coRP.}

\paragraph{ZPP as expected poly-time.}

\paragraph{IP.}

\paragraph{IP protocol for Graph-Non-Isomorphism.}

\paragraph{dIP=NP.}

\paragraph{coNP is contained in IP, arithmetization, and the sumcheck protocol.}

\paragraph{IP.}

\paragraph{Program checking.}

\paragraph{Zero knowledge proofs.}

\paragraph{Zero knowledge proof protocol for graph isomorphism.}

\paragraph{Communication Complexity.}

\paragraph{Computing OR.}

\paragraph{Computing MEDIAN.}

\paragraph{Communication Complexity.}

\paragraph{Computing MEDIAN.}

\paragraph{Protocol Trees.}

\paragraph{EQUALITY.}

\paragraph{Deciding Palindromes requires quadratic time.}

\paragraph{Communication Complexity.}

\paragraph{Protocol trees and combinatorial rectangles.}

\paragraph{EQUALITY (again).}

\paragraph{DISJOINTNESS.}

\paragraph{INNER PRODUCT and the rank technique.}

\paragraph{Area-Time tradeoffs for VLSI chips.}

\paragraph{NP-hard.}

\paragraph{Approximation Algorithms.}

\paragraph{2-approximation for Max Sat.}

\paragraph{2-approximation for Load Balancing (List Scheduling).}

\paragraph{3/2-approximation for Load Balancing (LPT).}

\paragraph{$O(\log n)$-approximation for Set Cover.}

\paragraph{2-approximation for Vertex Cover.}

\paragraph{PTAS and FPTAS.}

\paragraph{An FPTAS for Knapsack.}

\paragraph{Linear Programming based approximation algorithm for Weighted Set Cover.}

\end{document}
